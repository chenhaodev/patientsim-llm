{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14776835-04bd-4d31-bc09-096e9a7678e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from irrCAC.raw import CAC\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf508ae-45cc-4edf-94db-3fd9186facea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    with jsonlines.open(filename, \"r\") as file:\n",
    "        data_list = [line for line in file]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def get_profile(scenario_dict, trg_id):\n",
    "    for profile in scenario_dict:\n",
    "        if str(int(profile[\"hadm_id\"])) == str(int(trg_id)):\n",
    "            return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895ead86-c652-4b09-a1d5-d35cfefabf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \".\"\n",
    "display_order = ['gemini-2.5-flash-preview-04-17', 'gpt-4o-mini', 'vllm-deepseek-llama-70b', 'vllm-qwen2.5-72b-instruct', 'vllm-llama3.3-70b-instruct','vllm-llama3.1-70b-instruct',  'vllm-llama3.1-8b-instruct', 'vllm-qwen2.5-7b-instruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002701cb-ede6-46eb-9130-cda44173343f",
   "metadata": {},
   "source": [
    "## Patient Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2042f173-2089-445a-963b-9a0d599c80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "persona    108\n",
      "info        52\n",
      "valid       10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "diagnosis\n",
      "Intestinal obstruction     39\n",
      "Pneumonia                  34\n",
      "Urinary tract infection    34\n",
      "Myocardial infarction      34\n",
      "Cerebral infarction        29\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_profile = load_json(os.path.join(data_dir, \"patient_profile.json\"))\n",
    "patient_profile_df = pd.DataFrame(patient_profile)\n",
    "for _key in [\"split\", \"diagnosis\"]:\n",
    "    print(patient_profile_df[_key].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86226ae-c9fc-4004-897b-b9a7483cae2d",
   "metadata": {},
   "source": [
    "### RQ1: Do LLMs naturally reflect diverse persona traits in their responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a9c9af-f277-41d9-9654-88c42e0e23cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personality</th>\n",
       "      <th>cefr</th>\n",
       "      <th>recall</th>\n",
       "      <th>confused</th>\n",
       "      <th>realism</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_engine_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-flash-preview-04-17</th>\n",
       "      <td>3.94</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <td>3.58</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-deepseek-llama-70b</th>\n",
       "      <td>3.87</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-72b-instruct</th>\n",
       "      <td>3.30</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.3-70b-instruct</th>\n",
       "      <td>3.92</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.78</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-70b-instruct</th>\n",
       "      <td>3.65</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.62</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-8b-instruct</th>\n",
       "      <td>3.53</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-7b-instruct</th>\n",
       "      <td>3.23</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                personality  cefr  recall  confused  realism\n",
       "patient_engine_name                                                         \n",
       "gemini-2.5-flash-preview-04-17         3.94  3.54    3.64      3.38     3.37\n",
       "gpt-4o-mini                            3.58  3.55    3.78      3.88     3.26\n",
       "vllm-deepseek-llama-70b                3.87  3.58    3.42      2.50     3.19\n",
       "vllm-qwen2.5-72b-instruct              3.30  3.68    3.62      3.50     3.22\n",
       "vllm-llama3.3-70b-instruct             3.92  3.40    3.78      4.00     3.28\n",
       "vllm-llama3.1-70b-instruct             3.65  3.51    3.62      4.00     3.23\n",
       "vllm-llama3.1-8b-instruct              3.53  3.29    3.70      4.00     3.20\n",
       "vllm-qwen2.5-7b-instruct               3.23  3.49    3.31      3.50     3.16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_dir = os.path.join(data_dir, \"persona_test\", \"llm_simulation\")\n",
    "llm_persona_result_list = []\n",
    "for llm_backbone in os.listdir(persona_dir):\n",
    "    llm_result = load_jsonl(os.path.join(persona_dir, llm_backbone, \"llm_dialogue.jsonl\"))\n",
    "    llm_result = pd.DataFrame(llm_result)\n",
    "    llm_persona_result_list.append(llm_result)\n",
    "llm_persona_result_df = pd.concat(llm_persona_result_list)\n",
    "llm_persona_result_df.groupby(\"patient_engine_name\")[[\"personality\", \"cefr\", \"recall\", \"confused\", \"realism\"]].mean().round(2).loc[display_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2183bc5f-3ed4-412e-abf0-2435d590d0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_engine_name\n",
       "gemini-2.5-flash-preview-04-17    3.57\n",
       "gpt-4o-mini                       3.61\n",
       "vllm-deepseek-llama-70b           3.31\n",
       "vllm-qwen2.5-72b-instruct         3.46\n",
       "vllm-llama3.3-70b-instruct        3.68\n",
       "vllm-llama3.1-70b-instruct        3.60\n",
       "vllm-llama3.1-8b-instruct         3.54\n",
       "vllm-qwen2.5-7b-instruct          3.34\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_persona_result_df.groupby(\"patient_engine_name\")[[\"personality\", \"cefr\", \"recall\", \"confused\", \"realism\"]].mean().round(2).loc[display_order].mean(axis=1).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c43fc-74d2-4c8b-8a0e-3a61ced76374",
   "metadata": {},
   "source": [
    "### RQ2: Do LLMs accurately derive responses based on the given profile? & RQ3: Can LLMs reasonably fill in the blanks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a120a0-a452-4b5d-9e45-22bf73ee4c05",
   "metadata": {},
   "source": [
    "### Dialogue-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7d1c36-caac-4f28-8510-29ffbb5c85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(row, key):\n",
    "    if row[f\"{key}_gt\"] == \"Not recorded\" or row[f\"{key}_pred\"] == \"Not recorded\":\n",
    "        return 0\n",
    "    if key == \"pain\" and \"(predicted)\" in str(row[f\"{key}_pred\"]):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def compute_valid_column(df, key):\n",
    "    gt_col = f\"{key}_gt\"\n",
    "    pred_col = f\"{key}_pred\"\n",
    "    valid_col = f\"{key}_valid\"\n",
    "    if gt_col in df.columns and pred_col in df.columns:\n",
    "        df[valid_col] = df.apply(lambda row: is_valid(row, key), axis=1)\n",
    "    return df\n",
    "\n",
    "def aggregate_per_key(df, key, group_keys):\n",
    "    valid_col = f\"{key}_valid\"\n",
    "    llm_col = f\"{key}_llm\"\n",
    "    item_df = df[df[valid_col] == 1]\n",
    "\n",
    "    grouped_valid = item_df.groupby(group_keys).agg(\n",
    "        valid_count=(valid_col, \"count\"),\n",
    "        llm_score_mean=(llm_col, \"mean\")\n",
    "    ).reset_index()\n",
    "\n",
    "    total_counts = df.groupby(group_keys)[valid_col].count().reset_index(name=\"total_count\")\n",
    "\n",
    "    merged = pd.merge(grouped_valid, total_counts, on=group_keys, how=\"right\")\n",
    "    merged[\"valid_percentage\"] = merged[\"valid_count\"].fillna(0) / merged[\"total_count\"]\n",
    "    merged[\"item\"] = key\n",
    "    return merged\n",
    "\n",
    "def get_metric_table(df, value_col, metric_name, groupby_cols, ordered_cats):\n",
    "    grouped = df.groupby(groupby_cols)[value_col].mean().reset_index()\n",
    "    pivoted = grouped.pivot(index=groupby_cols[:1], columns=\"category\", values=value_col)\n",
    "    pivoted = pivoted.reindex(columns=ordered_cats)\n",
    "    # pivoted[\"metric\"] = metric_name\n",
    "    return pivoted.reset_index().set_index(groupby_cols[:1])\n",
    "    # return pivoted.reset_index().set_index(groupby_cols[:2] + [\"metric\"])\n",
    "\n",
    "\n",
    "def flatten_dict_simple(d, parent_key=\"\", sep=\"_\"):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        if parent_key == \"present_illness\":\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        else:\n",
    "            new_key = k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict_simple(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "eval_key_cat = {\n",
    "    \"Social_History\": ['tobacco', 'alcohol','illicit_drug', 'exercise', 'marital_status', 'children', 'living_situation', 'occupation'],\n",
    "    \"Previous_Medical_History\": ['allergies', 'family_medical_history', 'medical_device', 'medical_history'],\n",
    "    \"Current_Visit_Information\": ['chiefcomplaint', 'present_illness_positive', 'present_illness_negative', 'pain', 'medication',],\n",
    "}\n",
    "eval_key_to_cat = {key: category for category, keys in eval_key_cat.items() for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f21c1d7-2cfb-4cbc-b234-b326d8895b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dir = os.path.join(data_dir, \"info_test\", \"llm_simulation\")\n",
    "llm_info_result_list = []\n",
    "for llm_backbone in os.listdir(info_dir):\n",
    "    dialogue_data = load_jsonl(os.path.join(info_dir, llm_backbone, \"llm_dialogue.jsonl\"))\n",
    "    predict_profiles = load_json(os.path.join(info_dir, llm_backbone, \"gemini-2.5-flash-preview-04-17_profile_consistency_Patient.json\"))\n",
    "    LLM_score_dicts = load_json(os.path.join(info_dir, llm_backbone, \"gemini-2.5-flash-preview-04-17_profile_consistency_LLMscore_Patient.json\"))\n",
    "    for data in dialogue_data:\n",
    "        hadm_id = data[\"hadm_id\"]\n",
    "        profile_gt = get_profile(patient_profile, hadm_id)\n",
    "        predict_profile = flatten_dict_simple(predict_profiles[hadm_id])\n",
    "        \n",
    "        profile_gt = {k: v for k, v in profile_gt.items() if k in predict_profile.keys()}\n",
    "        LLM_dict = LLM_score_dicts[hadm_id]\n",
    "        LLM_score = {k: int(v[-1]) for k, v in LLM_dict.items()}\n",
    "        \n",
    "        meta_keys = [\"doctor_engine_name\", \"patient_engine_name\", \"cefr_type\",  \"personality_type\", \"recall_level_type\", \"dazed_level_type\"]\n",
    "        meta_data = {key: data.get(key) for key in meta_keys}\n",
    "        meta_data[\"hadm_id\"] = hadm_id\n",
    "        meta_df = pd.DataFrame([meta_data])\n",
    "        dfs = [\n",
    "            pd.DataFrame([profile_gt]).add_suffix(\"_gt\"),\n",
    "            pd.DataFrame([predict_profile]).add_suffix(\"_pred\"),\n",
    "            pd.DataFrame([LLM_score]).add_suffix(\"_llm\")\n",
    "        ]\n",
    "        sample_data = pd.concat([meta_df] + dfs, axis=1)\n",
    "\n",
    "        llm_info_result_list.append(sample_data)\n",
    "llm_info_result_df = pd.concat(llm_info_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff94106-7b4a-4212-a4d8-da084fc63484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>category</th>\n",
       "      <th>Social_History</th>\n",
       "      <th>Previous_Medical_History</th>\n",
       "      <th>Current_Visit_Information</th>\n",
       "      <th>Social_History</th>\n",
       "      <th>Previous_Medical_History</th>\n",
       "      <th>Current_Visit_Information</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_engine_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-flash-preview-04-17</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.72</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-deepseek-llama-70b</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-72b-instruct</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.3-70b-instruct</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.72</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-70b-instruct</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-8b-instruct</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.19</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-7b-instruct</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "category                        Social_History  Previous_Medical_History  \\\n",
       "patient_engine_name                                                        \n",
       "gemini-2.5-flash-preview-04-17            0.44                      0.77   \n",
       "gpt-4o-mini                               0.55                      0.76   \n",
       "vllm-deepseek-llama-70b                   0.50                      0.76   \n",
       "vllm-qwen2.5-72b-instruct                 0.47                      0.77   \n",
       "vllm-llama3.3-70b-instruct                0.53                      0.78   \n",
       "vllm-llama3.1-70b-instruct                0.56                      0.77   \n",
       "vllm-llama3.1-8b-instruct                 0.61                      0.78   \n",
       "vllm-qwen2.5-7b-instruct                  0.44                      0.75   \n",
       "\n",
       "category                        Current_Visit_Information  Social_History  \\\n",
       "patient_engine_name                                                         \n",
       "gemini-2.5-flash-preview-04-17                       0.88            3.82   \n",
       "gpt-4o-mini                                          0.89            3.72   \n",
       "vllm-deepseek-llama-70b                              0.91            3.73   \n",
       "vllm-qwen2.5-72b-instruct                            0.90            3.75   \n",
       "vllm-llama3.3-70b-instruct                           0.89            3.72   \n",
       "vllm-llama3.1-70b-instruct                           0.89            3.82   \n",
       "vllm-llama3.1-8b-instruct                            0.88            3.68   \n",
       "vllm-qwen2.5-7b-instruct                             0.89            3.60   \n",
       "\n",
       "category                        Previous_Medical_History  \\\n",
       "patient_engine_name                                        \n",
       "gemini-2.5-flash-preview-04-17                      3.51   \n",
       "gpt-4o-mini                                         3.33   \n",
       "vllm-deepseek-llama-70b                             3.31   \n",
       "vllm-qwen2.5-72b-instruct                           3.50   \n",
       "vllm-llama3.3-70b-instruct                          3.47   \n",
       "vllm-llama3.1-70b-instruct                          3.43   \n",
       "vllm-llama3.1-8b-instruct                           3.19   \n",
       "vllm-qwen2.5-7b-instruct                            3.32   \n",
       "\n",
       "category                        Current_Visit_Information  \n",
       "patient_engine_name                                        \n",
       "gemini-2.5-flash-preview-04-17                       3.18  \n",
       "gpt-4o-mini                                          3.01  \n",
       "vllm-deepseek-llama-70b                              3.08  \n",
       "vllm-qwen2.5-72b-instruct                            2.95  \n",
       "vllm-llama3.3-70b-instruct                           3.10  \n",
       "vllm-llama3.1-70b-instruct                           3.05  \n",
       "vllm-llama3.1-8b-instruct                            2.85  \n",
       "vllm-qwen2.5-7b-instruct                             2.89  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_keys = [\"patient_engine_name\"]\n",
    "groupby_cols = group_keys + [\"category\"]\n",
    "ordered_cats = [\"Social_History\", \"Previous_Medical_History\", \"Current_Visit_Information\"]\n",
    "metric_order = [\"valid_percentage\", \"bert_score_mean\", \"llm_score_mean\"]\n",
    "\n",
    "for key in profile_gt.keys():\n",
    "    llm_info_result_df = compute_valid_column(llm_info_result_df, key)\n",
    "\n",
    "grouped_results = [\n",
    "    aggregate_per_key(llm_info_result_df, key, group_keys)\n",
    "    for key in profile_gt.keys() if key in eval_key_to_cat\n",
    "]\n",
    "\n",
    "final_df = pd.concat(grouped_results, ignore_index=True)\n",
    "final_df[\"category\"] = final_df[\"item\"].map(eval_key_to_cat)\n",
    "\n",
    "valid_tbl = get_metric_table(final_df, \"valid_percentage\", \"valid_percentage\", groupby_cols, ordered_cats)\n",
    "llm_tbl = get_metric_table(final_df, \"llm_score_mean\", \"llm_score_mean\", groupby_cols, ordered_cats)\n",
    "final_stacked = pd.concat([valid_tbl, llm_tbl], axis=1)\n",
    "final_stacked = final_stacked.sort_index(axis=0, level=[1, 0])  # Sort by metric then category\n",
    "final_stacked = final_stacked.round(2)\n",
    "final_stacked.loc[display_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d35ae-6a51-43d2-88ac-2cb92d5c8245",
   "metadata": {},
   "source": [
    "### Sentence-level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3b01de-4d35-4a72-9e19-3897a732b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        for _k, _v in v.items():\n",
    "            items[_k] = _v \n",
    "    return items\n",
    "    \n",
    "def safe_div(numerator, denominator):\n",
    "    \"\"\"Safely divide two numbers, returning None if denominator is zero.\"\"\"\n",
    "    return numerator / denominator if denominator > 0 else None\n",
    "\n",
    "def calc_f1(precision, recall):\n",
    "    \"\"\"Calculate F1 score given precision and recall values.\"\"\"\n",
    "    return 2 * precision * recall / (precision + recall) if precision is not None and recall is not None and (precision + recall) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d71a45ef-a925-47b8-847f-311aef6fd2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info_frac</th>\n",
       "      <th>support_frac</th>\n",
       "      <th>unsupport_frac</th>\n",
       "      <th>entail_frac</th>\n",
       "      <th>contradict_frac</th>\n",
       "      <th>plausibility_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_engine_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-flash-preview-04-17</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.022</td>\n",
       "      <td>3.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <td>0.957</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.032</td>\n",
       "      <td>3.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-deepseek-llama-70b</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.032</td>\n",
       "      <td>3.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-72b-instruct</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.046</td>\n",
       "      <td>3.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.3-70b-instruct</th>\n",
       "      <td>0.958</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.019</td>\n",
       "      <td>3.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-70b-instruct</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.038</td>\n",
       "      <td>3.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-8b-instruct</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.056</td>\n",
       "      <td>3.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-7b-instruct</th>\n",
       "      <td>0.987</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.061</td>\n",
       "      <td>3.862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                info_frac  support_frac  unsupport_frac  \\\n",
       "patient_engine_name                                                       \n",
       "gemini-2.5-flash-preview-04-17      0.972         0.763           0.316   \n",
       "gpt-4o-mini                         0.957         0.721           0.428   \n",
       "vllm-deepseek-llama-70b             0.975         0.762           0.416   \n",
       "vllm-qwen2.5-72b-instruct           0.975         0.683           0.468   \n",
       "vllm-llama3.3-70b-instruct          0.958         0.796           0.387   \n",
       "vllm-llama3.1-70b-instruct          0.948         0.812           0.399   \n",
       "vllm-llama3.1-8b-instruct           0.944         0.771           0.488   \n",
       "vllm-qwen2.5-7b-instruct            0.987         0.703           0.453   \n",
       "\n",
       "                                entail_frac  contradict_frac  \\\n",
       "patient_engine_name                                            \n",
       "gemini-2.5-flash-preview-04-17        0.978            0.022   \n",
       "gpt-4o-mini                           0.968            0.032   \n",
       "vllm-deepseek-llama-70b               0.968            0.032   \n",
       "vllm-qwen2.5-72b-instruct             0.954            0.046   \n",
       "vllm-llama3.3-70b-instruct            0.981            0.019   \n",
       "vllm-llama3.1-70b-instruct            0.962            0.038   \n",
       "vllm-llama3.1-8b-instruct             0.944            0.056   \n",
       "vllm-qwen2.5-7b-instruct              0.939            0.061   \n",
       "\n",
       "                                plausibility_score  \n",
       "patient_engine_name                                 \n",
       "gemini-2.5-flash-preview-04-17               3.953  \n",
       "gpt-4o-mini                                  3.929  \n",
       "vllm-deepseek-llama-70b                      3.911  \n",
       "vllm-qwen2.5-72b-instruct                    3.928  \n",
       "vllm-llama3.3-70b-instruct                   3.963  \n",
       "vllm-llama3.1-70b-instruct                   3.958  \n",
       "vllm-llama3.1-8b-instruct                    3.897  \n",
       "vllm-qwen2.5-7b-instruct                     3.862  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_sentence_data(hadm_id, dialogue, nli_entry):\n",
    "    patient_utts = [d[\"content\"] for d in dialogue if d[\"role\"] == \"Patient\"]\n",
    "    assert set(nli_entry.keys()) == set(patient_utts)\n",
    "    total_utter_num = len(nli_entry)\n",
    "    sent_results = flatten_dict(nli_entry)\n",
    "    \n",
    "    stats = {\n",
    "        \"total_sent_num\": 0,\n",
    "        \"num_infomatic_sent\": 0,\n",
    "        \"num_entail\": 0,\n",
    "        \"num_support\": 0,\n",
    "        \"num_support_only\": 0,\n",
    "        \"num_unsupport\": 0,\n",
    "        \"num_unsupport_only\": 0,\n",
    "        \"both\": 0,\n",
    "        \"plausibility_score\": np.nan,\n",
    "        \"contradict_cnt\": 0,\n",
    "        \"contradict_result\": [],\n",
    "        \"plausibility_score_list\": [],\n",
    "        \"sent_category\": [],\n",
    "    }\n",
    "\n",
    "    plausibility_scores = []\n",
    "    contradict_results = []\n",
    "    sent_categories = []\n",
    "    for sent, result in sent_results.items():\n",
    "        stats[\"total_sent_num\"] += 1\n",
    "        sent_categories.append(result[\"step0\"][\"prediction\"])\n",
    "\n",
    "        if result[\"step0\"][\"prediction\"] != \"information\":\n",
    "            continue\n",
    "\n",
    "        stats[\"num_infomatic_sent\"] += 1\n",
    "\n",
    "        related_categories = [s[\"category\"] for s in result.get(\"step1-1\", []) if int(s[\"prediction\"]) == 1]\n",
    "        unsupport_flag = int(result.get(\"step1-2\", {}).get(\"prediction\", 0)) == 1\n",
    "        entail_dict = [s for s in result.get(\"step2-2\", []) if s[\"entailment_prediction\"] != 0]\n",
    "\n",
    "        is_unsupport = unsupport_flag or not entail_dict\n",
    "        if \"step2-1\" in result:\n",
    "            assert is_unsupport\n",
    "\n",
    "        if related_categories and entail_dict:\n",
    "            if any(s[\"entailment_prediction\"] == -1 for s in entail_dict):\n",
    "                contradict_results.append([s for s in result[\"step2-2\"] if s[\"entailment_prediction\"] == -1])\n",
    "            else:\n",
    "                stats[\"num_entail\"] += 1\n",
    "\n",
    "        if is_unsupport:\n",
    "            stats[\"num_unsupport\"] += 1\n",
    "            plausibility_scores.append(result[\"step2-1\"][\"likelihood_rating\"])\n",
    "            if not entail_dict:\n",
    "                stats[\"num_unsupport_only\"] += 1\n",
    "            else:\n",
    "                stats[\"num_support\"] += 1\n",
    "                stats[\"both\"] += 1\n",
    "        else:\n",
    "            stats[\"num_support\"] += 1\n",
    "            stats[\"num_support_only\"] += 1\n",
    "\n",
    "    stats[\"plausibility_score\"] = np.mean(plausibility_scores) if plausibility_scores else np.nan\n",
    "    stats[\"contradict_cnt\"] = len(contradict_results)\n",
    "    stats[\"contradict_result\"] = contradict_results\n",
    "    stats[\"plausibility_score_list\"] = plausibility_scores\n",
    "    stats[\"sent_category\"] = sent_categories\n",
    "    stats[\"total_utter_num\"] = total_utter_num\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def build_llm_info_sent_result_df(info_dir):\n",
    "    result_list = []\n",
    "    for llm_backbone in os.listdir(info_dir):\n",
    "        dialogue_data = load_jsonl(os.path.join(info_dir, llm_backbone, \"llm_dialogue.jsonl\"))\n",
    "        nli_data = load_json(os.path.join(info_dir, llm_backbone, \"gemini-2.5-flash-preview-04-17_sentence_label.json\"))\n",
    "\n",
    "        for data in dialogue_data:\n",
    "            hadm_id = data[\"hadm_id\"]\n",
    "            try:\n",
    "                stats = analyze_sentence_data(hadm_id, data[\"dialog_history\"], nli_data[hadm_id])\n",
    "            except:\n",
    "                print(hadm_id, data[\"patient_engine_name\"])\n",
    "            stats.update({\n",
    "                \"hadm_id\": hadm_id,\n",
    "                \"doctor_engine_name\": data[\"doctor_engine_name\"],\n",
    "                \"patient_engine_name\": data[\"patient_engine_name\"],\n",
    "            })\n",
    "            result_list.append(stats)\n",
    "\n",
    "    df = pd.DataFrame(result_list)\n",
    "\n",
    "    df[\"info_frac\"] = df[\"num_infomatic_sent\"] / df[\"total_sent_num\"]\n",
    "    df[\"support_frac\"] = df[\"num_support\"] / df[\"num_infomatic_sent\"]\n",
    "    df[\"entail_frac\"] = df[\"num_entail\"] / df[\"num_support\"]\n",
    "    df[\"contradict_frac\"] = df[\"contradict_cnt\"] / df[\"num_support\"]\n",
    "    df[\"unsupport_frac\"] = df[\"num_unsupport\"] / df[\"num_infomatic_sent\"]\n",
    "    df[\"both_frac\"] = df[\"both\"] / df[\"num_infomatic_sent\"]\n",
    "    df[\"unsupport_only\"] = df[\"unsupport_frac\"] - df[\"both_frac\"]\n",
    "    return df\n",
    "\n",
    "info_dir = os.path.join(data_dir, \"info_test\", \"llm_simulation\")\n",
    "llm_info_sent_result_df = build_llm_info_sent_result_df(info_dir)\n",
    "\n",
    "summary_df = llm_info_sent_result_df.groupby([\"patient_engine_name\"])[[\n",
    "    \"info_frac\", \"support_frac\", \"unsupport_frac\", \"entail_frac\", \"contradict_frac\", \"plausibility_score\"\n",
    "]].mean().round(3)\n",
    "summary_df.loc[display_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffba9f14-ba86-4abe-ae55-a3f55036cbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_utter_num</th>\n",
       "      <th>total_sent_num</th>\n",
       "      <th>num_infomatic_sent</th>\n",
       "      <th>num_support</th>\n",
       "      <th>num_unsupport</th>\n",
       "      <th>num_entail</th>\n",
       "      <th>contradict_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_engine_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-flash-preview-04-17</th>\n",
       "      <td>889</td>\n",
       "      <td>2286</td>\n",
       "      <td>2220</td>\n",
       "      <td>1695</td>\n",
       "      <td>705</td>\n",
       "      <td>1659</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <td>786</td>\n",
       "      <td>1937</td>\n",
       "      <td>1852</td>\n",
       "      <td>1331</td>\n",
       "      <td>795</td>\n",
       "      <td>1287</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-deepseek-llama-70b</th>\n",
       "      <td>806</td>\n",
       "      <td>1657</td>\n",
       "      <td>1614</td>\n",
       "      <td>1225</td>\n",
       "      <td>679</td>\n",
       "      <td>1186</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-72b-instruct</th>\n",
       "      <td>824</td>\n",
       "      <td>1820</td>\n",
       "      <td>1774</td>\n",
       "      <td>1201</td>\n",
       "      <td>839</td>\n",
       "      <td>1146</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.3-70b-instruct</th>\n",
       "      <td>806</td>\n",
       "      <td>2180</td>\n",
       "      <td>2087</td>\n",
       "      <td>1654</td>\n",
       "      <td>817</td>\n",
       "      <td>1623</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-70b-instruct</th>\n",
       "      <td>699</td>\n",
       "      <td>1946</td>\n",
       "      <td>1842</td>\n",
       "      <td>1493</td>\n",
       "      <td>745</td>\n",
       "      <td>1438</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-llama3.1-8b-instruct</th>\n",
       "      <td>742</td>\n",
       "      <td>1774</td>\n",
       "      <td>1672</td>\n",
       "      <td>1284</td>\n",
       "      <td>826</td>\n",
       "      <td>1210</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vllm-qwen2.5-7b-instruct</th>\n",
       "      <td>877</td>\n",
       "      <td>1579</td>\n",
       "      <td>1558</td>\n",
       "      <td>1092</td>\n",
       "      <td>712</td>\n",
       "      <td>1024</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                total_utter_num  total_sent_num  \\\n",
       "patient_engine_name                                               \n",
       "gemini-2.5-flash-preview-04-17              889            2286   \n",
       "gpt-4o-mini                                 786            1937   \n",
       "vllm-deepseek-llama-70b                     806            1657   \n",
       "vllm-qwen2.5-72b-instruct                   824            1820   \n",
       "vllm-llama3.3-70b-instruct                  806            2180   \n",
       "vllm-llama3.1-70b-instruct                  699            1946   \n",
       "vllm-llama3.1-8b-instruct                   742            1774   \n",
       "vllm-qwen2.5-7b-instruct                    877            1579   \n",
       "\n",
       "                                num_infomatic_sent  num_support  \\\n",
       "patient_engine_name                                               \n",
       "gemini-2.5-flash-preview-04-17                2220         1695   \n",
       "gpt-4o-mini                                   1852         1331   \n",
       "vllm-deepseek-llama-70b                       1614         1225   \n",
       "vllm-qwen2.5-72b-instruct                     1774         1201   \n",
       "vllm-llama3.3-70b-instruct                    2087         1654   \n",
       "vllm-llama3.1-70b-instruct                    1842         1493   \n",
       "vllm-llama3.1-8b-instruct                     1672         1284   \n",
       "vllm-qwen2.5-7b-instruct                      1558         1092   \n",
       "\n",
       "                                num_unsupport  num_entail  contradict_cnt  \n",
       "patient_engine_name                                                        \n",
       "gemini-2.5-flash-preview-04-17            705        1659              36  \n",
       "gpt-4o-mini                               795        1287              44  \n",
       "vllm-deepseek-llama-70b                   679        1186              39  \n",
       "vllm-qwen2.5-72b-instruct                 839        1146              55  \n",
       "vllm-llama3.3-70b-instruct                817        1623              31  \n",
       "vllm-llama3.1-70b-instruct                745        1438              55  \n",
       "vllm-llama3.1-8b-instruct                 826        1210              74  \n",
       "vllm-qwen2.5-7b-instruct                  712        1024              68  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_info_sent_result_df.groupby([\"patient_engine_name\"])[[\"total_utter_num\", \"total_sent_num\", \"num_infomatic_sent\", \"num_support\",  \"num_unsupport\", \"num_entail\", \"contradict_cnt\"]].sum().round().loc[display_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff884d1-a3ae-4f3d-ac81-407aade30abd",
   "metadata": {},
   "source": [
    "### Human Evaluation - Persona Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e253474-9a6e-4674-ade8-167807af9363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personality        3.96\n",
       "cefr               3.84\n",
       "recall             3.87\n",
       "confused           4.00\n",
       "realism            3.89\n",
       "tool_usefulness    3.75\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_dialog = load_jsonl(os.path.join(data_dir, \"persona_test\", \"expert_dialogue.jsonl\"))\n",
    "human_dialog_df = pd.DataFrame(human_dialog)\n",
    "human_dialog_df[[\"personality\", \"cefr\", \"recall\", \"confused\", \"realism\", \"tool_usefulness\"]].mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931705d0-4b59-417a-807e-ebafa7921c01",
   "metadata": {},
   "source": [
    "### Human Evaluation - Plausibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384a4d00-c819-4022-aae8-f5b771e96d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of labeled utterance (per labeler):  615.75\n",
      "# of labeled utterance:  821\n",
      "avg score:  3.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "labeler_name\n",
       "A    3.955\n",
       "B    3.923\n",
       "C    3.985\n",
       "D    3.781\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_plausibility_label = load_jsonl(os.path.join(data_dir, \"info_test\", \"expert_plausibility_label.jsonl\"))\n",
    "human_plausibility_label = pd.DataFrame(human_plausibility_label)\n",
    "print(\"# of labeled utterance (per labeler): \", human_plausibility_label.groupby(\"labeler_name\").utterance_id.nunique().mean())\n",
    "print(\"# of labeled utterance: \", human_plausibility_label.utterance_id.nunique())\n",
    "print(\"avg score: \", human_plausibility_label.score.mean().round(2))\n",
    "human_plausibility_label.groupby(\"labeler_name\").score.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c731b-b837-4455-a2b0-0a18225821b9",
   "metadata": {},
   "source": [
    "### Plausibility Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95ed3421-a949-41eb-b3f8-c2f57aaab1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rater_pair</th>\n",
       "      <th>agreement</th>\n",
       "      <th>mean</th>\n",
       "      <th>ci_lower_95%</th>\n",
       "      <th>ci_upper_95%</th>\n",
       "      <th>num_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-B</td>\n",
       "      <td>0.94917</td>\n",
       "      <td>0.948846</td>\n",
       "      <td>0.926880</td>\n",
       "      <td>0.968730</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-C</td>\n",
       "      <td>0.96819</td>\n",
       "      <td>0.968232</td>\n",
       "      <td>0.950790</td>\n",
       "      <td>0.982960</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-D</td>\n",
       "      <td>0.86583</td>\n",
       "      <td>0.865568</td>\n",
       "      <td>0.827517</td>\n",
       "      <td>0.901122</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-gemini-2.5-flash-preview-04-17</td>\n",
       "      <td>0.94371</td>\n",
       "      <td>0.943714</td>\n",
       "      <td>0.923410</td>\n",
       "      <td>0.960430</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-C</td>\n",
       "      <td>0.96161</td>\n",
       "      <td>0.960926</td>\n",
       "      <td>0.939590</td>\n",
       "      <td>0.978520</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-D</td>\n",
       "      <td>0.85360</td>\n",
       "      <td>0.853072</td>\n",
       "      <td>0.818096</td>\n",
       "      <td>0.885670</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-gemini-2.5-flash-preview-04-17</td>\n",
       "      <td>0.94459</td>\n",
       "      <td>0.944609</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>0.961090</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C-D</td>\n",
       "      <td>0.87824</td>\n",
       "      <td>0.878705</td>\n",
       "      <td>0.842892</td>\n",
       "      <td>0.912820</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C-gemini-2.5-flash-preview-04-17</td>\n",
       "      <td>0.96398</td>\n",
       "      <td>0.963905</td>\n",
       "      <td>0.947330</td>\n",
       "      <td>0.977180</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D-gemini-2.5-flash-preview-04-17</td>\n",
       "      <td>0.88266</td>\n",
       "      <td>0.882590</td>\n",
       "      <td>0.857062</td>\n",
       "      <td>0.907741</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         rater_pair  agreement      mean  ci_lower_95%  \\\n",
       "0                               A-B    0.94917  0.948846      0.926880   \n",
       "1                               A-C    0.96819  0.968232      0.950790   \n",
       "2                               A-D    0.86583  0.865568      0.827517   \n",
       "3  A-gemini-2.5-flash-preview-04-17    0.94371  0.943714      0.923410   \n",
       "4                               B-C    0.96161  0.960926      0.939590   \n",
       "5                               B-D    0.85360  0.853072      0.818096   \n",
       "6  B-gemini-2.5-flash-preview-04-17    0.94459  0.944609      0.926200   \n",
       "7                               C-D    0.87824  0.878705      0.842892   \n",
       "8  C-gemini-2.5-flash-preview-04-17    0.96398  0.963905      0.947330   \n",
       "9  D-gemini-2.5-flash-preview-04-17    0.88266  0.882590      0.857062   \n",
       "\n",
       "   ci_upper_95%  num_sample  \n",
       "0      0.968730        1000  \n",
       "1      0.982960        1000  \n",
       "2      0.901122        1000  \n",
       "3      0.960430        1000  \n",
       "4      0.978520        1000  \n",
       "5      0.885670        1000  \n",
       "6      0.961090        1000  \n",
       "7      0.912820        1000  \n",
       "8      0.977180        1000  \n",
       "9      0.907741        1000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_pairwise_agreement_with_ci(long_df, rater_col=\"labeler_name\", agreement_type=\"exact_agreement\", max_score=4, n_bootstrap=1000, ci=95, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    pivot = long_df.pivot_table(index='utterance_id', columns=rater_col, values='score')\n",
    "    raters = pivot.columns\n",
    "    results = []\n",
    "\n",
    "    for r1, r2 in combinations(raters, 2):\n",
    "        sub = pivot[[r1, r2]].dropna()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        if agreement_type == \"exact_agreement\":\n",
    "            def stat_fn(df): return (df[r1] == df[r2]).mean()\n",
    "        elif agreement_type == \"exact_mse_norm\":\n",
    "            def stat_fn(df): return 1 - (df[r1] - df[r2]).abs().mean() / (max_score - 1)\n",
    "        elif agreement_type == \"gwets_ac1\":\n",
    "            def stat_fn(df):\n",
    "                try:\n",
    "                    cac = CAC(df, categories=list(range(1, max_score + 1)), weights='identity')\n",
    "                    return cac.gwet()[\"est\"][\"coefficient_value\"]\n",
    "                except Exception:\n",
    "                    return np.nan  \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown agreement_type: {agreement_type}\")\n",
    "        \n",
    "\n",
    "        point_estimate = stat_fn(sub)\n",
    "\n",
    "        # Bootstrap\n",
    "        scores = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample_df = sub.sample(n=len(sub), replace=True)\n",
    "            score = stat_fn(sample_df)\n",
    "            if not np.isnan(score):\n",
    "                scores.append(score)\n",
    "        try:\n",
    "            lower = np.percentile(scores, (100 - ci) / 2)\n",
    "            upper = np.percentile(scores, 100 - (100 - ci) / 2)\n",
    "        except:\n",
    "            return sub\n",
    "\n",
    "        results.append({\n",
    "            \"rater_pair\": f\"{r1}-{r2}\",\n",
    "            \"agreement\": point_estimate,\n",
    "            f\"mean\": np.mean(scores),\n",
    "            f\"ci_lower_{ci}%\": lower,\n",
    "            f\"ci_upper_{ci}%\": upper,\n",
    "            f\"num_sample\": len(scores)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "llm_plausibility_label = load_jsonl(os.path.join(data_dir, \"info_test\", \"llm_plausibility_label.jsonl\"))\n",
    "llm_plausibility_label = pd.DataFrame(llm_plausibility_label)\n",
    "human_llm_plausibility_label = pd.concat([llm_plausibility_label, human_plausibility_label]).reset_index(drop=True)\n",
    "agreement_ci = compute_pairwise_agreement_with_ci(human_llm_plausibility_label, agreement_type=\"gwets_ac1\")\n",
    "agreement_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98a2d09-2048-4b35-a488-310dba5a315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_DESCRIPTION = {\n",
    "    \"age\": \"Age: {age}\",\n",
    "    \"gender\": \"Gender: {gender}\",\n",
    "    \"race\": \"Race: {race}\",\n",
    "    \"tobacco\": \"Tobacco: {tobacco}\",\n",
    "    \"alcohol\": \"Alcohol: {alcohol}\",\n",
    "    \"illicit_drug\": \"Illicit drug use: {illicit_drug}\",\n",
    "    \"sexual_history\": \"Sexual History: {sexual_history}\",\n",
    "    \"exercise\": \"Exercise: {exercise}\",\n",
    "    \"marital_status\": \"Marital status: {marital_status}\",\n",
    "    \"children\": \"Children: {children}\",\n",
    "    \"living_situation\": \"Living Situation: {living_situation}\",\n",
    "    \"occupation\": \"Occupation: {occupation}\",\n",
    "    \"insurance\": \"Insurance: {insurance}\",\n",
    "    \"allergies\": \"Allergies: {allergies}\",\n",
    "    \"family_medical_history\": \"Family medical history: {family_medical_history}\",\n",
    "    \"medical_device\": \"Medical devices previously used or currently in use before this ED admission: {medical_device}\",\n",
    "    \"medical_history\": \"Medical history prior to this ED admission: {medical_history}\",\n",
    "    \"present_illness\": \"Present illness:\\n\\tpositive: {present_illness_positive}\\n\\tnegative (denied): {present_illness_negative}\",\n",
    "    \"chief_complaint\": \"ED chief complaint: {chiefcomplaint}\",\n",
    "    \"pain\": \"Pain level at ED Admission (0 = no pain, 10 = worst pain imaginable): {pain}\",\n",
    "    \"medication\": \"Current medications they are taking: {medication}\",\n",
    "    \"arrival_transport\": \"ED Arrival Transport: {arrival_transport}\",\n",
    "    \"diagnosis\": \"ED Diagnosis: {diagnosis}\",\n",
    "}\n",
    "\n",
    "def reverse_map_key(target_sentence):\n",
    "    for key, value_template in KEY_DESCRIPTION.items():\n",
    "        prefix = value_template.split(\"{\")[0].strip()  # {    \n",
    "        if target_sentence.startswith(prefix):\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_step0(human_label, model_pred):\n",
    "    is_info = human_label == \"information\"\n",
    "    is_correct = human_label == model_pred\n",
    "    is_tp = is_info and is_correct\n",
    "    return is_info, is_tp, is_correct\n",
    "\n",
    "def evaluate_step2(human_step2, model_step2):\n",
    "    model_preds = {\n",
    "        reverse_map_key(item[\"profile\"]): item[\"entailment_prediction\"]\n",
    "        for item in model_step2 if item[\"entailment_prediction\"] != 0\n",
    "    }\n",
    "    human_labels = {k: 1 if v == \"e\" else -1 for k, v in human_step2.items()}\n",
    "    eval_metrics = evaluate_prediction(human_labels, model_preds)\n",
    "    return eval_metrics, set(model_preds.keys()), set(human_labels.keys())\n",
    "\n",
    "def is_unsupported_pred(utterance_data):\n",
    "    return \"step2-1\" in utterance_data\n",
    "    \n",
    "def evaluate_sets(gt_set, pred_set):\n",
    "    intersection = len(gt_set & pred_set)\n",
    "    precision = intersection / len(pred_set) if pred_set else 0\n",
    "    recall = intersection / len(gt_set) if gt_set else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1score\": round(f1_score, 4)\n",
    "    }\n",
    "def evaluate_prediction(ans, pred):\n",
    "    ans_keys = set(ans.keys())\n",
    "    pred_keys = set(pred.keys())\n",
    "    common_keys = ans_keys.intersection(pred_keys)\n",
    "    \n",
    "    key_eval = evaluate_sets(ans_keys, pred_keys)\n",
    "    correct_values = sum(1 for key in common_keys if ans[key] == pred[key])\n",
    "    value_accuracy = correct_values / len(common_keys) if common_keys else 0\n",
    "    value_tp = 0\n",
    "    for key, value in ans.items():\n",
    "        if value == pred.get(key):\n",
    "            value_tp += 1\n",
    "\n",
    "    return {\n",
    "        'key_precision': key_eval[\"Precision\"],\n",
    "        'key_recall': key_eval[\"Recall\"],\n",
    "        'key_f1': key_eval[\"F1score\"],\n",
    "        'value_accuracy': value_accuracy,\n",
    "        \"value_recall\": value_tp / len(ans_keys) if len(ans_keys) > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49c6371f-f244-434f-82d5-d5091e2b7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>llm</th>\n",
       "      <th>sentence_label_gemini-2.5-flash-preview-04-17.json</th>\n",
       "      <th>sentence_label_gpt-4o.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_sentences</th>\n",
       "      <td>41.10</td>\n",
       "      <td>41.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step0_accuracy</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step0_recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step2_macro_avg_key_precision</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step2_macro_avg_key_recall</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step2_macro_avg_key_f1</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step2_macro_avg_value_accuracy</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step2_macro_avg_value_recall</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsupported_precision</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsupported_recall</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsupported_f1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "llm                             sentence_label_gemini-2.5-flash-preview-04-17.json  \\\n",
       "total_sentences                                                             41.10    \n",
       "step0_accuracy                                                               0.96    \n",
       "step0_recall                                                                 0.99    \n",
       "step2_macro_avg_key_precision                                                0.90    \n",
       "step2_macro_avg_key_recall                                                   0.96    \n",
       "step2_macro_avg_key_f1                                                       0.92    \n",
       "step2_macro_avg_value_accuracy                                               0.98    \n",
       "step2_macro_avg_value_recall                                                 0.96    \n",
       "unsupported_precision                                                        0.84    \n",
       "unsupported_recall                                                           0.86    \n",
       "unsupported_f1                                                               0.84    \n",
       "\n",
       "llm                             sentence_label_gpt-4o.json  \n",
       "total_sentences                                      41.10  \n",
       "step0_accuracy                                        0.94  \n",
       "step0_recall                                          0.98  \n",
       "step2_macro_avg_key_precision                         0.92  \n",
       "step2_macro_avg_key_recall                            0.94  \n",
       "step2_macro_avg_key_f1                                0.92  \n",
       "step2_macro_avg_value_accuracy                        0.97  \n",
       "step2_macro_avg_value_recall                          0.94  \n",
       "unsupported_precision                                 0.89  \n",
       "unsupported_recall                                    0.64  \n",
       "unsupported_f1                                        0.74  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_label_human = load_json(os.path.join(data_dir, \"sentence_cls_valid\", \"sentence_label_manual.json\"))\n",
    "dialogue_hists = load_jsonl(os.path.join(data_dir, \"sentence_cls_valid\", \"dialogue.jsonl\"))\n",
    "\n",
    "print(len(dialogue_hists))\n",
    "print(len(sentence_label_human))\n",
    "valid_result_dict = []\n",
    "for llm_path in [\"sentence_label_gemini-2.5-flash-preview-04-17.json\", \"sentence_label_gpt-4o.json\"]:\n",
    "    nli_data = load_json(os.path.join(data_dir, \"sentence_cls_valid\", llm_path))\n",
    "    \n",
    "    for data in dialogue_hists:\n",
    "        hadm_id = data[\"hadm_id\"]\n",
    "        counters = {\n",
    "            \"total_sentences\": 0,\n",
    "            \"step0_correct\": 0,\n",
    "            \"step0_tp_sum\": 0,\n",
    "            \"step0_gt_info\": 0,\n",
    "            \"step2_gt_count\": 0,\n",
    "            \"step2_pred_count\": 0,\n",
    "            \"step2_eval_count\": 0,\n",
    "            \"step2_key_precision_sum\": 0,\n",
    "            \"step2_key_recall_sum\": 0,\n",
    "            \"step2_key_f1_sum\": 0,\n",
    "            \"step2_value_correct\": 0,\n",
    "            \"step2_value_recall\": 0,\n",
    "            \"unsupported_gt_count\": 0,\n",
    "            \"unsupported_pred_count\": 0,\n",
    "            \"unsupported_tp\": 0,\n",
    "            \"unsupported_only_gt_count\": 0,\n",
    "            \"unsupported_only_pred_count\": 0,\n",
    "        }\n",
    "\n",
    "        dialogue_info_tp = 0\n",
    "        dialogue_info_pred_total = 0\n",
    "        dialogue_info_gt_total = 0\n",
    "\n",
    "        for utter in data[\"dialog_history\"]:\n",
    "            if utter[\"role\"] != \"Patient\":\n",
    "                continue\n",
    "            utterance = utter[\"content\"]\n",
    "\n",
    "            if utterance not in nli_data[hadm_id] or utterance not in sentence_label_human[hadm_id]:\n",
    "                print(utterance)\n",
    "                continue\n",
    "\n",
    "            utterance_data = nli_data[hadm_id][utterance]\n",
    "            human_utterance_data = sentence_label_human[hadm_id][utterance]\n",
    "\n",
    "            for sent, model_info in utterance_data.items():\n",
    "                if sent not in human_utterance_data:\n",
    "                    continue\n",
    "\n",
    "                counters[\"total_sentences\"] += 1\n",
    "                human_step0 = human_utterance_data[sent][\"step0\"]\n",
    "                model_step0 = \"information\" if model_info[\"step0\"][\"prediction\"] == \"information\" else \"non-information\"\n",
    "\n",
    "                is_info, is_tp, is_correct = evaluate_step0(human_step0, model_step0)\n",
    "                counters[\"step0_gt_info\"] += is_info\n",
    "                counters[\"step0_tp_sum\"] += is_tp\n",
    "                counters[\"step0_correct\"] += is_correct\n",
    "\n",
    "                if \"step2-2\" in human_utterance_data[sent]:\n",
    "                    counters[\"step2_gt_count\"] += 1\n",
    "                if \"step2-2\" in model_info:\n",
    "                    counters[\"step2_pred_count\"] += 1\n",
    "                if \"step2-2\" in human_utterance_data[sent] and \"step2-2\" in model_info:\n",
    "                    counters[\"step2_eval_count\"] += 1\n",
    "\n",
    "                # Step1: unsupported\n",
    "                if is_tp:\n",
    "                    step1_unsupported_human = \"unsupported\" in human_utterance_data[sent][\"step1\"]\n",
    "                    step1_unsupported_pred = is_unsupported_pred(model_info)\n",
    "\n",
    "                    counters[\"unsupported_gt_count\"] += step1_unsupported_human\n",
    "                    counters[\"unsupported_only_gt_count\"] += step1_unsupported_human and not step1_unsupported_pred\n",
    "                    counters[\"unsupported_only_pred_count\"] += step1_unsupported_pred and not step1_unsupported_human\n",
    "                    counters[\"unsupported_tp\"] += step1_unsupported_human and step1_unsupported_pred\n",
    "                    counters[\"unsupported_pred_count\"] += step1_unsupported_pred\n",
    "\n",
    "                    # Step2: fine-grained eval\n",
    "                    if \"step2-2\" in human_utterance_data[sent] and \"step2-2\" in model_info:\n",
    "                        eval_metrics, pred_keys, gt_keys = evaluate_step2(human_utterance_data[sent][\"step2-2\"], model_info[\"step2-2\"])\n",
    "\n",
    "                        counters[\"step2_key_precision_sum\"] += eval_metrics[\"key_precision\"]\n",
    "                        counters[\"step2_key_recall_sum\"] += eval_metrics[\"key_recall\"]\n",
    "                        counters[\"step2_key_f1_sum\"] += eval_metrics[\"key_f1\"]\n",
    "                        counters[\"step2_value_correct\"] += eval_metrics[\"value_accuracy\"]\n",
    "                        counters[\"step2_value_recall\"] += eval_metrics[\"value_recall\"]\n",
    "\n",
    "                        dialogue_info_tp += len(pred_keys & gt_keys)\n",
    "                        dialogue_info_pred_total += len(pred_keys)\n",
    "                        dialogue_info_gt_total += len(gt_keys)\n",
    "\n",
    "        if counters[\"total_sentences\"] > 0:\n",
    "            metrics = {\n",
    "                \"llm\": llm_path.replace(\"_nli_Patient.json\", \"\"),\n",
    "                \"total_sentences\": counters[\"total_sentences\"],\n",
    "                \"step0_accuracy\": safe_div(counters[\"step0_correct\"], counters[\"total_sentences\"]),\n",
    "                \"step0_recall\": safe_div(counters[\"step0_tp_sum\"], counters[\"step0_gt_info\"]),\n",
    "                \"step2_macro_avg_key_precision\": safe_div(counters[\"step2_key_precision_sum\"], counters[\"step2_eval_count\"]),\n",
    "                \"step2_macro_avg_key_recall\": safe_div(counters[\"step2_key_recall_sum\"], counters[\"step2_eval_count\"]),\n",
    "                \"step2_macro_avg_key_f1\": safe_div(counters[\"step2_key_f1_sum\"], counters[\"step2_eval_count\"]),\n",
    "                \"step2_macro_avg_value_accuracy\": safe_div(counters[\"step2_value_correct\"], counters[\"step2_eval_count\"]),\n",
    "                \"step2_macro_avg_value_recall\": safe_div(counters[\"step2_value_recall\"], counters[\"step2_eval_count\"]),\n",
    "                \"unsupported_precision\": safe_div(counters[\"unsupported_tp\"], counters[\"unsupported_pred_count\"]),\n",
    "                \"unsupported_recall\": safe_div(counters[\"unsupported_tp\"], counters[\"unsupported_gt_count\"]),\n",
    "                \"unsupported_f1\": calc_f1(\n",
    "                    safe_div(counters[\"unsupported_tp\"], counters[\"unsupported_pred_count\"]),\n",
    "                    safe_div(counters[\"unsupported_tp\"], counters[\"unsupported_gt_count\"])\n",
    "                )\n",
    "            }\n",
    "            valid_result_dict.append(metrics)\n",
    "\n",
    "sample_df = pd.DataFrame(valid_result_dict)\n",
    "sample_df.groupby(\"llm\").mean().T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b51594-74a3-40f6-8a06-8f7a5cc7e875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
